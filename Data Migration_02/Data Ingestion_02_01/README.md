# Connecting On-Prem SQL Server to Azure Data Factory

With both our source and target databases set up, I started connecting the on-premises SQL Server database to Azure Data Factory (ADF). Since ADF does not directly connect to on-premises databases, I used **Integration Runtime (IR)** for this. Specifically, **Self-Hosted Integration Runtime (SHIR)** was set up to securely transfer data between the on-premises source and the Azure cloud using ADF's Linked Service.

## Steps to Connect On-Premises SQL Server to ADF

1. **Access Integration Runtimes in ADF Studio:**
   - Open the **Manage** tab in ADF Studio and select **Integration Runtimes**.
   
2. **Create a New Integration Runtime:**
   - Click **New** and choose **Self-hosted Integration Runtime (SHIR)**, as our data source is an on-premise SQL Server.
   - Name the integration runtime and click **Create**.

3. **Install SHIR:**
   - For this project, I used the Express setup to install SHIR directly on my machine.
   - After downloading the SHIR setup, I installed and registered it on the local system, enabling ADF to securely communicate with the on-premises SQL Server.

4. **Verify Connection:**
   - Open Microsoft Integration Runtime on the local system to confirm that it shows as connected to the ADF environment.

With the databases now connected, the next step was configuring a data pipeline to transfer data from the source to the target.

---

# Configuring the Data Ingestion Pipeline

### Step 1: Create a New Pipeline

1. In the **Author** tab of ADF, select **Pipelines** and create a new pipeline named "CopyPipeline" to represent the data copy operation.
2. Add a **Copy Data** activity to the pipeline to pull data from the on-premises SQL Server.
3. Define the source and target (sink) for the pipeline.

---

## Specifying Source and Sink

### Source Configuration

- In the **Copy Data** activity, navigate to **Source** and select **New Dataset**.
  
  ![Source for Ingestion](./img/Source%20for%20Data%20Ingestion.png)
  
- Choose **SQL Server** as the dataset type and connect it to the on-premises SQL Server using **Linked Services**.
- Create a **New Linked Service** to configure the connection properties:
  - Select **SHIR** as the Integration Runtime.
  
    ![SQL Server Linked Service Screenshot](./img/SQL%20Linked%20Service.png)
    
  - Enter the server and database names from SQL Server Management Studio.
  - For authentication, I utilized the credentials stored in our **Azure Key Vault**. This required creating a new linked service for the Key Vault, allowing secure communication between ADF and the Key Vault.
  
    ![Azure Key Vault Linked Service Image](./img/Key%20Vault%20Linked%20Service.png)

- These linked services enable ADF to securely connect to the on-premises SQL Server.

### Sink Configuration

- In the **Copy Data** activity, navigate to **Sink**.
  
  ![Sink for Ingestion](./img/Sink%20for%20Ingestion.png)
  
- **Sink** is set to the `sourcelayer` container within the **sddatalakegen2** resource.
- Created an **Azure Data Lake Storage Linked Service** named **AzureDataLakeStorage1**. Here, **AutoResolveIntegrationRuntime** was used since the resource is cloud-based. I specified the file path and source container as shown below:
  
  ![Datalake Linked Service Screenshot](./img/DataLake%20Linked%20Service.png)
  
- After completing these configurations, I published all changes to ADF, establishing a fully connected setup for data ingestion.

---

# Defining the Data Pipeline

With the source and sink configured, I proceeded to define the pipeline's actions. Below is a screenshot of the overall pipeline layout.

![Pipeline Image](./img/Pipeline.png)

1. **Look Up Activity**: This initial activity retrieves a list of all table names in the on-premises database.
2. **For Each Activity**: The list of tables generated by the Look Up activity is passed to this activity, which processes each table in sequence. The activities inside the **For Each** task are shown below.

   ![Inside For Each Table](./img/Inside%20For%20Each%20Table.png)
   
   ![Inside For Each Table](./img/Inside%20For%20Each%20Table%202.png)

3. The **For Each** activity is configured to copy each table from the source to the target database.

---

## Source and Sink for 'For Each Task'

The **source** and **sink** for the **For Each** activity are shown in the screenshots below.

- **Source Configuration in For Each Task**:

  ![Source Inside For Each Task](./img/Source%20Inside%20For%20Each%20Task.png)

- **Sink Configuration in For Each Task**:

  ![Sink Inside For Each Task](./img/Sink%20Inside%20For%20Each%20Task.png)

The data ingestion portion of the pipeline is now complete with these two activities.

---

# Running the Pipeline

By running the pipeline, we now have an exact copy of all tables from the **POS Schema** on the on-premises SQL Server. These tables are stored in the **sourcelayer** container in **parquet** file format, as shown below.

- **Source Layer Folder Screenshot**:

  ![Source Layer Folder Screenshot](./img/Source%20Layer%20Folders%20Screenshot.png)

- **POS Database in Source Layer**:

  ![POS Database Inside Source Layer](./img/Source%20Layer%20Parquet%20Files%20Screenshot.png)

- **City Table in Parquet Format**:

  ![City Parquet File Screenshot](./img/City%20Parquet%20File%20Screenshot.png)

---

### ETL Project Structure

To maintain a consistent project structure across ETL projects, I created separate folders for the database inside the **source layer**. This structure is typically used by companies for all ETL projects. If additional databases were included, they could be organized similarly, ensuring that each layer (Source, Stage, and Fact) remains consistent and organized.